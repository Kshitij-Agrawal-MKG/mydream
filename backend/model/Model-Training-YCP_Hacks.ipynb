{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# YCP Hacks 2024\n",
        "***Team: Sanij, Priyanshu, Andry, Marko (SPAM)***\n",
        "\n",
        "This is the codespace where we trained a transfer learning model for predicting different skin conditions using the **Dermnet** dataset (https://www.kaggle.com/datasets/shubhamgoel27/dermnet/code).\n",
        "\n"
      ],
      "metadata": {
        "id": "SdyVrFR1aChi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "J8NncKDet02u"
      },
      "outputs": [],
      "source": [
        "# importing the libraries (do not remove or add libraries)\n",
        "from typing import List, Set, Dict, Tuple, Optional\n",
        "from PIL import Image\n",
        "import os\n",
        "import pathlib\n",
        "import cv2\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
        "from tensorflow.keras.models import Model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"shubhamgoel27/dermnet\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GANw9sLn9hV1",
        "outputId": "9ab53f06-aad8-4b81-cb93-482abce8636c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/shubhamgoel27/dermnet?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.72G/1.72G [00:59<00:00, 31.2MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/shubhamgoel27/dermnet/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading data"
      ],
      "metadata": {
        "id": "-o_K59jMDfma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = path+'/train'\n",
        "\n",
        "train_data = []\n",
        "val_data = []\n",
        "\n",
        "for folder in os.listdir(data_path):\n",
        "    folder_path = os.path.join(data_path, folder)\n",
        "    file = os.listdir(folder_path)\n",
        "    num_train = int(0.8 * len(file))\n",
        "    files_train = random.sample(file, num_train)\n",
        "    files_val = list(set(file) - set(files_train))\n",
        "\n",
        "    for file in files_train:\n",
        "        file_path = os.path.join(folder_path, file)\n",
        "        img = cv2.imread(file_path)\n",
        "        img = cv2.resize(img, (224,224))\n",
        "        train_data.append((img, folder))\n",
        "\n",
        "    for file in files_val:\n",
        "        file_path = os.path.join(folder_path, file)\n",
        "        img = cv2.imread(file_path)\n",
        "        img = cv2.resize(img, (224,224))\n",
        "        val_data.append((img, folder))"
      ],
      "metadata": {
        "id": "r7Jtn3_-DfTj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up model and Train"
      ],
      "metadata": {
        "id": "DOplT-TuDkhv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: set up the imported data and train it on a MobileNetV2 transfer learning model\n",
        "\n",
        "# Define the image size and batch size\n",
        "IMG_WIDTH = 224\n",
        "IMG_HEIGHT = 224\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "base_model = tf.keras.applications.MobileNetV2(input_shape=(IMG_WIDTH, IMG_HEIGHT, 3),\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "base_model.trainable = False\n",
        "num_classes = 23\n",
        "\n",
        "x = GlobalAveragePooling2D()(base_model.output)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "predictions = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "transfer_model = Model(inputs=base_model.input, outputs=predictions)\n",
        "transfer_model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#preprocess data\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "#train_data = [(preprocess_input(input), label) for input, label in train_data]\n",
        "#val_data = [(preprocess_input(input), label) for input, label in val_data]\n",
        "\n",
        "X_train, y_train = zip(*train_data)\n",
        "X_val, y_val = zip(*val_data)\n",
        "\n",
        "X_train = preprocess_input(np.array(X_train))\n",
        "X_val = preprocess_input(np.array(X_val))\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_train_encoded = le.fit_transform(y_train)\n",
        "y_val_encoded = le.transform(y_val)\n",
        "\n",
        "y_train_one_hot = to_categorical(y_train_encoded, num_classes)\n",
        "y_val_one_hot = to_categorical(y_val_encoded, num_classes)\n",
        "\n",
        "\n",
        "# Train the model\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "custom_early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    min_delta=0.001,\n",
        "    mode='min'\n",
        ")\n",
        "\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 64\n",
        "history = transfer_model.fit(X_train, y_train_one_hot, validation_data=(X_val, y_val_one_hot),\n",
        "                   epochs = EPOCHS, batch_size=BATCH_SIZE,callbacks=[custom_early_stopping])\n"
      ],
      "metadata": {
        "id": "ZKs3C2R0VDc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f516e2ec-0c33-4f8e-cdcb-7aabb47f0d85"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Epoch 1/20\n",
            "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 146ms/step - accuracy: 0.2423 - loss: 2.6679 - val_accuracy: 0.3206 - val_loss: 2.3172\n",
            "Epoch 2/20\n",
            "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 68ms/step - accuracy: 0.3833 - loss: 2.0446 - val_accuracy: 0.3549 - val_loss: 2.2301\n",
            "Epoch 3/20\n",
            "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 69ms/step - accuracy: 0.4776 - loss: 1.7471 - val_accuracy: 0.3713 - val_loss: 2.1747\n",
            "Epoch 4/20\n",
            "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 68ms/step - accuracy: 0.5279 - loss: 1.5462 - val_accuracy: 0.3937 - val_loss: 2.1049\n",
            "Epoch 5/20\n",
            "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 68ms/step - accuracy: 0.6096 - loss: 1.2938 - val_accuracy: 0.4075 - val_loss: 2.1153\n",
            "Epoch 6/20\n",
            "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 69ms/step - accuracy: 0.6782 - loss: 1.0822 - val_accuracy: 0.4113 - val_loss: 2.1787\n",
            "Epoch 7/20\n",
            "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 69ms/step - accuracy: 0.7549 - loss: 0.8614 - val_accuracy: 0.4178 - val_loss: 2.2174\n",
            "Epoch 8/20\n",
            "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 69ms/step - accuracy: 0.8073 - loss: 0.6829 - val_accuracy: 0.4174 - val_loss: 2.3070\n",
            "Epoch 9/20\n",
            "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 69ms/step - accuracy: 0.8489 - loss: 0.5340 - val_accuracy: 0.4255 - val_loss: 2.3322\n",
            "Epoch 10/20\n",
            "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 69ms/step - accuracy: 0.8906 - loss: 0.4188 - val_accuracy: 0.4206 - val_loss: 2.4766\n",
            "Epoch 11/20\n",
            "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 69ms/step - accuracy: 0.9222 - loss: 0.3257 - val_accuracy: 0.4229 - val_loss: 2.4747\n",
            "Epoch 12/20\n",
            "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 70ms/step - accuracy: 0.9427 - loss: 0.2550 - val_accuracy: 0.4197 - val_loss: 2.5591\n",
            "Epoch 13/20\n",
            "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 70ms/step - accuracy: 0.9535 - loss: 0.2185 - val_accuracy: 0.4271 - val_loss: 2.6493\n",
            "Epoch 14/20\n",
            "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 69ms/step - accuracy: 0.9637 - loss: 0.1739 - val_accuracy: 0.4296 - val_loss: 2.7086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transfer_model.save('tf_model.keras')"
      ],
      "metadata": {
        "id": "GIFn7huDXgIa"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}